{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Raw Data (with Sentences and Ground Truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: ..\\data\\raw_data_with_gt.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>chunk</th>\n",
       "      <th>ner_label</th>\n",
       "      <th>label_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Unfortunately , I think itis unlikelythathis s...</td>\n",
       "      <td>['he', 'over 9 to 12 months', 'he', 'he']</td>\n",
       "      <td>[Gender, RelativeDate, Gender, Gender]</td>\n",
       "      <td>{'Gender': 3, 'RelativeDate': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0  50  Unfortunately , I think itis unlikelythathis s...   \n",
       "\n",
       "                                       chunk  \\\n",
       "0  ['he', 'over 9 to 12 months', 'he', 'he']   \n",
       "\n",
       "                                ner_label                       label_count  \n",
       "0  [Gender, RelativeDate, Gender, Gender]  {'Gender': 3, 'RelativeDate': 1}  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_name = 'raw_data_with_gt.pkl'\n",
    "data_path = os.path.join('..', 'data', file_name )\n",
    "print(\"Raw data path:\", data_path)\n",
    "data_df = pd.read_pickle(data_path)\n",
    "data_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Task Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction Tasks**\n",
    "1. 'Organ_Component'\n",
    "   <br> *Evaluated for* \n",
    "    - External_body_part_or_region\n",
    "    - Internal_organ_or_component\n",
    "2. 'Gender'\n",
    "3. 'Procedure_Treatment'\n",
    "    <br> *Evaluated for* \n",
    "    - Procedure\n",
    "    - Treatment\n",
    "4. 'Problem' (19 Entities merged under Problem Type)\n",
    "5. 'Medicine' (Drug_BrandName, Drug_Ingredient, Strength, Form, Dosage )\n",
    "<br> In this evaluation case partial matches are considered as full_match\n",
    "6. 'Drug'\n",
    "    <br> *Evaluated for* \n",
    "    - Drug (Drug_BrandName, Drug_Ingredient)\n",
    "    - Drug_Related (Strength, Form, Dosage)\n",
    "7. 'Test'\n",
    "8. 'Test_TestResults' (Test, Test Results)\n",
    "    <br> *Evaluated for* \n",
    "    - Test\n",
    "    - Test_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_list: dict_keys(['Deid', 'Organ_Component', 'Gender', 'Procedure_Treatment', 'Problem', 'Medicine', 'Drug', 'Test', 'Test_TestResult', 'Oncological'])\n",
      "eval_options_dict: {'External_body_part_or_region': {'selected_entity_prediction': ['External_body_part_or_region'], 'selected_entity_gt': ['External_body_part_or_region'], 'gt_type_dict': {'External_body_part_or_region': 'External_body_part_or_region'}}, 'Internal_organ_or_component': {'selected_entity_prediction': ['Internal_organ_or_component'], 'selected_entity_gt': ['Internal_organ_or_component'], 'gt_type_dict': {'Internal_organ_or_component': 'Internal_organ_or_component'}}, 'Gender': {'selected_entity_prediction': ['Gender'], 'selected_entity_gt': ['Gender'], 'gt_type_dict': {'Gender': 'Gender'}}, 'Procedure': {'selected_entity_prediction': ['Procedure'], 'selected_entity_gt': ['Procedure'], 'gt_type_dict': {'Procedure': 'Procedure'}}, 'Treatment': {'selected_entity_prediction': ['Treatment'], 'selected_entity_gt': ['Treatment'], 'gt_type_dict': {'Treatment': 'Treatment'}}, 'Problem': {'selected_entity_prediction': ['Problem'], 'selected_entity_gt': ['Psychological_Condition', 'Symptom', 'Disease_Syndrome_Disorder', 'Heart_Disease', 'Oncological', 'Kidney_Disease', 'Overweight', 'Diabetes', 'Obesity', 'EKG_Findings', 'Communicable_Disease', 'Tumor_Finding', 'ImagingFindings', 'Cerebrovascular_Disease', 'Psychological_Condition', 'Hyperlipidemia', 'VS_Finding', 'Hypertension', 'Metastasis'], 'gt_type_dict': {'Psychological_Condition': 'Problem', 'Symptom': 'Problem', 'Disease_Syndrome_Disorder': 'Problem', 'Heart_Disease': 'Problem', 'Oncological': 'Problem', 'Kidney_Disease': 'Problem', 'Overweight': 'Problem', 'Diabetes': 'Problem', 'Obesity': 'Problem', 'EKG_Findings': 'Problem', 'Communicable_Disease': 'Problem', 'Tumor_Finding': 'Problem', 'ImagingFindings': 'Problem', 'Cerebrovascular_Disease': 'Problem', 'Hyperlipidemia': 'Problem', 'VS_Finding': 'Problem', 'Hypertension': 'Problem', 'Metastasis': 'Problem'}}, 'Medicine': {'selected_entity_prediction': ['Medicine'], 'selected_entity_gt': ['Drug_BrandName', 'Drug_Ingredient', 'Strength', 'Dosage', 'Form'], 'gt_type_dict': {'Drug_BrandName': 'Medicine', 'Drug_Ingredient': 'Medicine', 'Strength': 'Medicine', 'Dosage': 'Medicine', 'Form': 'Medicine'}}, 'Drug': {'selected_entity_prediction': ['Drug'], 'selected_entity_gt': ['Drug_BrandName', 'Drug_Ingredient'], 'gt_type_dict': {'Drug_BrandName': 'Drug', 'Drug_Ingredient': 'Drug'}}, 'Drug_Related': {'selected_entity_prediction': ['DrugRelated'], 'selected_entity_gt': ['Strength', 'Dosage', 'Form'], 'gt_type_dict': {'Strength': 'DrugRelated', 'Dosage': 'DrugRelated', 'Form': 'DrugRelated'}}, 'Test': {'selected_entity_prediction': ['Test'], 'selected_entity_gt': ['Test'], 'gt_type_dict': {'Test': 'Test'}}, 'Test_Result': {'selected_entity_prediction': ['Test_Result'], 'selected_entity_gt': ['Test_Result'], 'gt_type_dict': {'Test_Result': 'Test_Result'}}, 'Oncological_Granular': {'selected_entity_prediction': ['Oncological'], 'selected_entity_gt': ['Tumor_Finding', 'Site_Lymph_Node', 'Adenopathy', 'Cancer_Dx', 'Cancer_Score', 'Cancer_Surgery', 'Chemotherapy', 'Grade', 'Metastasis', 'Pathology_Result', 'Pathology_Test', 'Staging'], 'gt_type_dict': {'Tumor_Finding': 'Oncological', 'Site_Lymph_Node': 'Oncological', 'Adenopathy': 'Oncological', 'Cancer_Dx': 'Oncological', 'Cancer_Score': 'Oncological', 'Cancer_Surgery': 'Oncological', 'Chemotherapy': 'Oncological', 'Grade': 'Oncological', 'Metastasis': 'Oncological', 'Pathology_Result': 'Oncological', 'Pathology_Test': 'Oncological', 'Staging': 'Oncological'}}, 'Deid': {'selected_entity_prediction': ['ID', 'DATE', 'AGE', 'PHONE', 'PERSON', 'LOCATION', 'ORGANIZATION'], 'selected_entity_gt': ['ID', 'DATE', 'AGE', 'PHONE', 'DOCTOR', 'PATIENT', 'NAME', 'HOSPITAL', 'LOCATION', 'ORGANIZATION'], 'gt_type_dict': {'ID': 'ID', 'LOCATION': 'LOCATION', 'DATE': 'DATE', 'AGE': 'AGE', 'PATIENT': 'PERSON', 'DOCTOR': 'PERSON', 'NAME': 'PERSON', 'HOSPITAL': 'ORGANIZATION', 'ORGANIZATION': 'ORGANIZATION'}}}\n"
     ]
    }
   ],
   "source": [
    "from modules.ProcessRawData import ProcessData\n",
    "from modules.ProcessPredData import corrected_json, get_list_of_entities\n",
    "import json\n",
    "\n",
    "# read tasks from file\n",
    "with open(\"./sources/tasks_list.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# extract problem_entity_list and task_list\n",
    "task_list = data[\"task_list\"]\n",
    "eval_options_dict = data[\"eval_options_dict\"]\n",
    "\n",
    "print(\"task_list:\", task_list.keys())\n",
    "print(\"eval_options_dict:\",eval_options_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Select Task to Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c7633405334eaf8dfcd86e6455e29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Deid', 'Organ_Component', 'Gender', 'Procedure_Treatment', 'Problem', 'Medicine', 'Drug', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc44ab4548d40f6bc2dac35ae9c969d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Approve', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions will be generated for the entities: Oncological\n",
      "Predictions will be generated for the entities: Oncological\n"
     ]
    }
   ],
   "source": [
    "## !!! USER SELECTION NECESSARY in this cell\n",
    "\n",
    "# # select entity for evaluation\n",
    "# entity_options = list(task_list.keys())\n",
    "# entity_prompt = \"Please select an entity for prediction generatio (options are: {}): \".format(\", \".join(entity_options))\n",
    "# entity_under_test = input(entity_prompt)\n",
    "\n",
    "# # check if the user input is valid\n",
    "# while entity_under_test not in entity_options:\n",
    "#     print(\"Invalid input. Please try again.\")\n",
    "#     entity_under_test = input(entity_prompt)\n",
    "\n",
    "# # continue with evaluation using selected entity_under_test\n",
    "# print(\"Predictions will be generated for the entities:\", entity_under_test)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# select entity for evaluation\n",
    "entity_options = list(task_list.keys())\n",
    "\n",
    "def assign_entity(entity):\n",
    "    global entity_under_test\n",
    "    entity_under_test = entity\n",
    "    print(\"Predictions will be generated for the entities:\", entity_under_test)\n",
    "\n",
    "# create the dropdown menu\n",
    "dropdown = widgets.Dropdown(options=entity_options)\n",
    "\n",
    "# create the button\n",
    "button = widgets.Button(description=\"Approve\")\n",
    "\n",
    "# define the callback function for the button\n",
    "def on_button_click(button):\n",
    "    assign_entity(dropdown.value)\n",
    "\n",
    "# register the callback function with the button\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# display the dropdown menu and the button\n",
    "display(dropdown)\n",
    "display(button)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 02:22:55,165 - ProcessRawData - INFO - main df shape: (849, 5)\n",
      "2023-11-06 02:22:55,167 - ProcessRawData - INFO - filtered df shape: (0, 6)\n",
      "2023-11-06 02:22:55,168 - ProcessRawData - INFO - final df shape: (0, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing main csv...\n"
     ]
    }
   ],
   "source": [
    "# filter gt data for prediction based on selected entity under test and entity counts\n",
    "covered_gt_label_list = task_list[entity_under_test]['covered_gt_label_list']\n",
    "count_filter = task_list[entity_under_test]['count_filter']\n",
    "\n",
    "prediction_source = \"ChatGPT\"\n",
    "\n",
    "# process gt data for prediction and evaluation\n",
    "process_gt_data = ProcessData(\n",
    "    covered_gt_label_list, \n",
    "    count_filter, \n",
    "    data_path,\n",
    "    prediction_source\n",
    ")\n",
    "\n",
    "processed_gt_df = process_gt_data.get_final_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GPT Predictions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load Prompt and Initiate Entity Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt for [Oncological] extraction task: (.\\prompts\\11_02_Oncological.txt)\n",
      "PROMPT:\n",
      "\n",
      "\n",
      "\n",
      "You are a highly experienced and skilled medical annotator who have been working on medical texts to label medical entities.\n",
      "\n",
      "\n",
      "\n",
      "I will provide you some entity types with sample chunks and I want you to find similar entities from given texts and label them with right entity types.\n",
      "\n",
      "\n",
      "\n",
      "-  Entity Type: Oncological\n",
      "\n",
      "    Instruction: include all the cancer, tumor or metastasis related extractions mentioned in the document, of the patient or someone else.\n",
      "\n",
      "\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    a) given sample sentence:\n",
      "\n",
      "    His mother was diagnosed with colon cancer in her 50s, but she died of cancer of the esophagus at age 86.\n",
      "\n",
      "    Oncological Entities in above given text: colon cancer, cancer of the esophagus\n",
      "\n",
      "\n",
      "\n",
      "    b) given sample sentence:\n",
      "\n",
      "    She was diagnosed with pseudomyxoma peritonei in 1994.\n",
      "\n",
      "    Oncological Entities in above given text: pseudomyxoma peritonei\n",
      "\n",
      "\n",
      "\n",
      "I want you to extract all Oncological type entitie and chunks from the given text one-by-one and them label one-by-one .\n",
      "\n",
      "\n",
      "\n",
      "Task :\n",
      "\n",
      "\n",
      "\n",
      "Find entities in the given sentence.\n",
      "\n",
      "\n",
      "\n",
      "Answer value must be as given (valid JSON) for the given example sentence:\n",
      "\n",
      "\n",
      "\n",
      "{{\"given_sentence\": \"Father got a mesothelioma at age 65\",\n",
      "\n",
      "    \"list_of_entities\":\n",
      "\n",
      "    [\n",
      "\n",
      "        {{\"entity_type\": \"Oncological\", \"chunk\": \"mesothelioma\"}}\n",
      "\n",
      "    ]\n",
      "\n",
      "}}\n",
      "\n",
      "\n",
      "\n",
      "Now I want you to find the Oncological entities in the given sentence:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from modules import NerExtraction\n",
    "prompt_file_path = os.path.join('.', 'prompts', task_list[entity_under_test]['prompt_file'] )\n",
    "print(f\"Prompt for [{entity_under_test}] extraction task: ({prompt_file_path})\")\n",
    "\n",
    "with open(prompt_file_path, 'r') as file:\n",
    "    print(\"PROMPT:\\n\")\n",
    "    for line in file:\n",
    "        print(line)\n",
    "    \n",
    "ner_extraction = NerExtraction.ChatGPTNER(prompt_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\processed_data\\Oncological_preds_1106_0222\n",
      "Given dataframe shape (0, 8)\n",
      "Getting predictions from API started...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ground_truth_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ground_truth_list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Projects\\spark-nlp-workshop\\tutorials\\academic\\LLMs_in_Healthcare\\benchmarks\\workbench\\01_00_Reproduce_Benchmarks_NER_for_GPT.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Projects/spark-nlp-workshop/tutorials/academic/LLMs_in_Healthcare/benchmarks/workbench/01_00_Reproduce_Benchmarks_NER_for_GPT.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(processed_data_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Projects/spark-nlp-workshop/tutorials/academic/LLMs_in_Healthcare/benchmarks/workbench/01_00_Reproduce_Benchmarks_NER_for_GPT.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m batch_processor \u001b[39m=\u001b[39m BatchProcessing\u001b[39m.\u001b[39mProcessBatch(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Projects/spark-nlp-workshop/tutorials/academic/LLMs_in_Healthcare/benchmarks/workbench/01_00_Reproduce_Benchmarks_NER_for_GPT.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     processed_gt_df,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Projects/spark-nlp-workshop/tutorials/academic/LLMs_in_Healthcare/benchmarks/workbench/01_00_Reproduce_Benchmarks_NER_for_GPT.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     ner_extraction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Projects/spark-nlp-workshop/tutorials/academic/LLMs_in_Healthcare/benchmarks/workbench/01_00_Reproduce_Benchmarks_NER_for_GPT.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     processed_data_path\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Projects/spark-nlp-workshop/tutorials/academic/LLMs_in_Healthcare/benchmarks/workbench/01_00_Reproduce_Benchmarks_NER_for_GPT.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Projects/spark-nlp-workshop/tutorials/academic/LLMs_in_Healthcare/benchmarks/workbench/01_00_Reproduce_Benchmarks_NER_for_GPT.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m results_df \u001b[39m=\u001b[39m batch_processor\u001b[39m.\u001b[39;49mdo_processing()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Projects\\spark-nlp-workshop\\tutorials\\academic\\LLMs_in_Healthcare\\benchmarks\\workbench\\modules\\BatchProcessing.py:42\u001b[0m, in \u001b[0;36mProcessBatch.do_processing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     41\u001b[0m df_filtered \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mprediction_list\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)]\n\u001b[1;32m---> 42\u001b[0m df_filtered \u001b[39m=\u001b[39m df_filtered[df_filtered[\u001b[39m'\u001b[39;49m\u001b[39mground_truth_list\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)]\n\u001b[0;32m     43\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     df_filtered\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_name\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ground_truth_list'"
     ]
    }
   ],
   "source": [
    "# !!! Run without any changes\n",
    "\n",
    "from modules import BatchProcessing\n",
    "from modules.ProcessPredData import corrected_json, get_list_of_entities\n",
    "import datetime\n",
    "\n",
    "# Assing auto name to save prediction data as csv and excel\n",
    "now = datetime.datetime.now()\n",
    "file_name = f\"{entity_under_test}_preds_{now.strftime('%m%d_%H%M')}\"\n",
    "processed_data_path = os.path.join('.', 'processed_data', file_name)\n",
    "\n",
    "print(processed_data_path)\n",
    "batch_processor = BatchProcessing.ProcessBatch(\n",
    "    processed_gt_df,\n",
    "    ner_extraction,\n",
    "    corrected_json,\n",
    "    get_list_of_entities,\n",
    "    processed_data_path\n",
    ")\n",
    "\n",
    "results_df = batch_processor.do_processing()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Selection of entity for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_list_for_benchmark: ['Test', 'Test_Result']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa97e1b15c740648cd838bbdb2898aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Test', 'Test_Result'), value='Test')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c942f05e55c441da58651ccd33d0aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Approve', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarks will be evaluated for the entity: Test\n",
      "Benchmarks will be evaluated for the entity: Test_Result\n"
     ]
    }
   ],
   "source": [
    "###!!! USER INPUT MIGHT BE NECESSARY if there are multiple entites for evaluation under given task\n",
    "## selection of entity type for benchmark\n",
    "\n",
    "# entity_list_for_benchmark = data[\"entity_for_benchmark\"][entity_under_test]\n",
    "# print(\"entity_list_for_benchmark:\", entity_list_for_benchmark)\n",
    "# if len(entity_list_for_benchmark) == 1:\n",
    "#     entity_for_benchmark = entity_list_for_benchmark[0]\n",
    "#     print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "# else:\n",
    "#     entity_prompt = \"Please select an entity for evaluation (options are: {}): \".format(\", \".join(entity_list_for_benchmark))\n",
    "#     entity_for_benchmark = input(entity_prompt)\n",
    "#     print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# selection of entity type for benchmark\n",
    "entity_list_for_benchmark = data[\"entity_for_benchmark\"][entity_under_test]\n",
    "print(\"entity_list_for_benchmark:\", entity_list_for_benchmark)\n",
    "\n",
    "if len(entity_list_for_benchmark) == 1:\n",
    "    entity_for_benchmark = entity_list_for_benchmark[0]\n",
    "    print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "else:\n",
    "    def assign_entity(entity):\n",
    "        global entity_for_benchmark\n",
    "        entity_for_benchmark = entity\n",
    "        print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "    \n",
    "    # create the dropdown menu\n",
    "    dropdown = widgets.Dropdown(options=entity_list_for_benchmark)\n",
    "\n",
    "    # create the button\n",
    "    button = widgets.Button(description=\"Approve\")\n",
    "\n",
    "    # define the callback function for the button\n",
    "    def on_button_click(button):\n",
    "        assign_entity(dropdown.value)\n",
    "\n",
    "    # register the callback function with the button\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "    # display the dropdown menu and the button\n",
    "    display(dropdown)\n",
    "    display(button)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Run Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved as ./eval_results/Test_Result_ChatGPT_eval_1106_0220.xlsx\n",
      "Results appended to file: eval_results/eval_result.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'version': 'Test_Result_ChatGPT_eval_1106_0220',\n",
       " 'selected_entity_prediction': ['Test_Result'],\n",
       " 'selected_entity_gt': ['Test_Result'],\n",
       " 'full_match': 277,\n",
       " 'accuracy_full_match': 0.76,\n",
       " 'partial_match': 30,\n",
       " 'accuracy_partial_match': 0.84,\n",
       " 'no_match': 59,\n",
       " 'gt_count': 366,\n",
       " 'fp_count': 42}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No user interaction needed, just run the cell\n",
    "from modules import Evaluation\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "eval_file_to_save = entity_for_benchmark + \"_\" + prediction_source + f\"_eval_{now.strftime('%m%d_%H%M')}\"\n",
    "\n",
    "selected_entity_prediction = eval_options_dict[entity_for_benchmark][\"selected_entity_prediction\"]\n",
    "selected_entity_gt = eval_options_dict[entity_for_benchmark][\"selected_entity_gt\"]\n",
    "gt_type_dict = eval_options_dict[entity_for_benchmark][\"gt_type_dict\"]\n",
    "\n",
    "file_path__to_read_prediction = f\"{processed_data_path}.csv\"\n",
    "# alternative to reading prediction results from file \n",
    "# dataframe = results_df !!! make >> file_path__to_read_prediction = None \n",
    "\n",
    "evaluator = Evaluation.Evaluate(\n",
    "    file_path=file_path__to_read_prediction, \n",
    "    dataframe=None, \n",
    "    prediction_source=prediction_source\n",
    ")\n",
    "\n",
    "eval_results = evaluator.get_match_counts(\n",
    "    selected_entity_prediction, \n",
    "    selected_entity_gt, \n",
    "    eval_file_to_save,\n",
    "    gt_type_dict\n",
    ")\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "if not os.path.exists('eval_results'):\n",
    "    os.makedirs('eval_results')\n",
    "    \n",
    "# write result to JSON file\n",
    "with open('eval_results/eval_result.json', 'a') as f:\n",
    "    json.dump(eval_results, f)\n",
    "    print(\"Results appended to file: eval_results/eval_result.json\")\n",
    "    \n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
